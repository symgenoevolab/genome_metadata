{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cff2f794-0e21-472b-8815-6e763af5982d",
   "metadata": {},
   "source": [
    "### Helper functions: \n",
    "These are the necessary helper functions to run the three metadata processing files. This file does not actually execute any commands but defines the necessary functions and classes to be executed in the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "750c041d-8e05-4de7-9aa5-2cc6c60fec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in a filepath to a tsv file for information pulled from NCBI using dataset/dataformat. \n",
    "# Read it using pandas, rename the columns to make it easier to work with, and set the index to the accession number. \n",
    "def processNcbiMetadata(filepath): \n",
    "    my_data = pd.read_table(filepath)\n",
    "    my_data = renameColumns(my_data) \n",
    "    my_data = my_data.set_index('Accession', drop = True) \n",
    "    return(my_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fce5f0e4-1f35-4722-9ab1-9ab81c9da9ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prune the metadata to keep only one assembly per species, selecting for the earliest publication date. \n",
    "def metadataEarliest(metadata): \n",
    "    earliest = (metadata.sort_values('First_Publication_Date')\n",
    "                        .drop_duplicates('Organism_Name', keep = 'first'))\n",
    "    earliest = earliest.set_index('Accession', drop = True) \n",
    "    return(earliest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01f9320d-db83-4c48-9550-08bb5e377715",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prune the metadata to keep only one assembly per species, selecting for the most recent publication date. \n",
    "def metadataNewest(metadata, sort_by = 'Release_Date'): \n",
    "    newest = (metadata.sort_values(sort_by, ascending = True)\n",
    "                      .drop_duplicates('Organism_Name', keep = 'first'))\n",
    "    newest = newest.set_index('Accession', drop = True) \n",
    "    return(newest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccfdb707-d517-47be-88ce-f5950905191c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns to make it easier to work with. \n",
    "# Remove the substrings \"Assembly \" and \"Stats \" from the column for more concise descriptors. \n",
    "# Replace space with underscore such that columns can be referenced using Python's dot syntax. \n",
    "def renameColumns(dataframe): \n",
    "    newNames = {}\n",
    "    for colName in dataframe.columns: \n",
    "        newName = (colName.replace('Assembly ', '')\n",
    "                          .replace('Stats ', '')\n",
    "                          .replace(' ', '_'))\n",
    "        newNames[colName] = newName\n",
    "    dataframe = dataframe.rename(columns = newNames)\n",
    "    return(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfee125b-f526-49e8-a42b-2cfa1989e3a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new column based on the \"Sequencing_Tech\" column to simplify the information into sequencing type (long or short). \n",
    "def readType(metadata): \n",
    "    metadata['Sequencing_Tech'] = metadata['Sequencing_Tech'].astype(str) \n",
    "    long_reads = ['pacbio', 'nanopore']\n",
    "    short_reads = ['illumina', 'hi-c', 'hic', 'iontorrent', 'sanger', 'hiseq', '10x', '454'] \n",
    "    metadata['Sequencing_Type'] = 'No information provided' \n",
    "    is_short = [any(company in sequencing_type for company in short_reads)\n",
    "                for sequencing_type in metadata.Sequencing_Tech.str.lower()] \n",
    "    metadata.loc[is_short, 'Sequencing_Type'] = 'Short read' \n",
    "    is_long = [any(company in sequencing_type for company in long_reads) \n",
    "               for sequencing_type in metadata.Sequencing_Tech.str.lower()]\n",
    "    metadata.loc[is_long, 'Sequencing_Type'] = 'Long read'\n",
    "\n",
    "    \n",
    "    return(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0091e657-2bba-46cd-a5be-760392149660",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the datatypes for columns of a pandas dataframe. \n",
    "def prepareMetadata(metadata): \n",
    "    dates = [column for column in metadata.columns if 'Date' in column]\n",
    "    for column in dates: \n",
    "        metadata[column] = pd.to_datetime(metadata[column])\n",
    "    return(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab8f8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the clade of an NCBI record based on the clade name.\n",
    "def get_clade(record, clade): \n",
    "    for entry in record['LineageEx']: \n",
    "        if clade in entry.values(): \n",
    "            return entry['ScientificName']  \n",
    "    return 'N/A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5460d3bf-729b-4149-80e7-e40be8dc21b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking in a Pandas DataFrame of NCBI metadata, crawl the NCBI website to find any extra information that may not be as easy to find using command line tools. \n",
    "\n",
    "class Metadata: \n",
    "    def __init__(self, my_data): \n",
    "        self.table = my_data\n",
    "        self.accessions = self.table.index\n",
    "        self.data = {}\n",
    "        self.tax_data = {}\n",
    "    \n",
    "    ## Find the ncbi page for each accession in the given table and pull all the information into self.data\n",
    "    def threadCreep(self): \n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor: \n",
    "            results = [executor.submit(self.addAccession, accession) for accession in self.accessions]\n",
    "            done = 0    \n",
    "            for f in concurrent.futures.as_completed(results): \n",
    "                done = done + 1 \n",
    "                print(f'\\rComplete: {done}/{len(self.table)}', end = '\\r')\n",
    "                sys.stdout.flush()\n",
    "        \n",
    "    def taxCreep(self): \n",
    "        self.table['Phylum'] = 'N/A'\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor: \n",
    "            results = [executor.submit(self.getPhyla, accession) for accession in self.accessions] \n",
    "            done = 0 \n",
    "            for f in concurrent.futures.as_completed(results): \n",
    "                done = done + 1\n",
    "                print(f'\\rComplete: {done}/{len(self.table)}', end = '\\r')\n",
    "                sys.stdout.flush()\n",
    "            \n",
    "    def firstPub(self, date_column = 'Release_Date'): \n",
    "        # isolate accessions ending in characters other than 1 to check for original publications \n",
    "        multi_pub_candidates = self.accessions[[accession[-1] != '1' for accession in self.accessions]]\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor: \n",
    "            first_pub = {accession : executor.submit(self.getFirstPub, accession) for accession in multi_pub_candidates} \n",
    "            done = 0    \n",
    "            for f in concurrent.futures.as_completed(first_pub.values()): \n",
    "                done += 1 \n",
    "                print(f'\\rComplete: {done}/{len(multi_pub_candidates)}', end = '\\r')\n",
    "                sys.stdout.flush()\n",
    "                sys.stdout.flush()\n",
    "            first_pub = {accession : future.result() for accession, future in first_pub.items()}\n",
    "            \n",
    "            # Cut out any entries where the first pub accession just results in an empty list\n",
    "            first_pub = {accession : accession_list for accession, accession_list in first_pub.items() if accession_list}\n",
    "            \n",
    "            first_pub_date = {accession : self.lookFor('Date', accession_list)\n",
    "                                              .replace('/', '-') for accession, accession_list in first_pub.items()} \n",
    "            first_contig_n50 = {accession: self.lookFor('Contig N50', accession_list)\n",
    "                                               .replace(',', '') for accession, accession_list in first_pub.items()} \n",
    "            self.table['First_Publication_Date'] = self.table[date_column].replace(first_pub_date)\n",
    "            self.table['Original_Contig_N50'] = self.table['Contig_N50'].replace(first_contig_n50)\n",
    "            self.first_pub = first_pub\n",
    "                \n",
    "        \n",
    "    def getPhyla(self, accession): \n",
    "        url = self.getTaxURL(self.data[accession][1]) \n",
    "        page = requests.get(url).content.decode() \n",
    "        tax_list = self.webFilter(page) \n",
    "        tax_list = [word \n",
    "                    for line in tax_list \n",
    "                    for word in line.split()]\n",
    "        tax_list = [line for line in tax_list if 'TITLE=\"' in line]\n",
    "        self.tax_data[accession] = tax_list\n",
    "        phylum_line = [line for line in tax_list if 'TITLE=\"phylum\"' in line] \n",
    "        if len(phylum_line)==1: \n",
    "            phylum_line = phylum_line[0]\n",
    "            start_id = phylum_line.find('>') + 1\n",
    "            phylum = phylum_line[start_id:] \n",
    "            #print(phylum) \n",
    "        else: \n",
    "            phylum = \"n/a\" \n",
    "        self.table.loc[accession, 'Phylum'] = phylum\n",
    "        \n",
    "    def getFirstPub(self, accession): \n",
    "        first_accession = accession[:len(accession)-1] + '1' \n",
    "        first_accList = self.getAccList(first_accession) \n",
    "        return(first_accList) \n",
    "            \n",
    "    def lookFor(self, request, searchList): \n",
    "        if request in searchList: \n",
    "            index = searchList.index(request) + 1\n",
    "            info = self.removeLink(searchList[index]) \n",
    "            return(info) \n",
    "        else: \n",
    "            return('N/A')\n",
    "        \n",
    "    ### Remove hyperlink from HTML format \n",
    "    def removeLink(self, string): \n",
    "        if string.find('<a') == -1: \n",
    "            return string\n",
    "        else: \n",
    "            string = string[string.find('>') + 1:]\n",
    "            return string\n",
    "        \n",
    "    def getTaxURL(self, string): \n",
    "        urlStart = string.find('\"') + 1\n",
    "        string = string[urlStart:]\n",
    "        urlEnd = string.find('\"')\n",
    "        string = string[:urlEnd]\n",
    "        url = \"https://www.ncbi.nlm.nih.gov\" + string\n",
    "        return(url)\n",
    "    \n",
    "    def addAccession(self, accession): \n",
    "        accList = self.getAccList(accession)\n",
    "        self.data[accession] = accList \n",
    "        \n",
    "    def getAccList(self, accession): \n",
    "        url = 'https://www.ncbi.nlm.nih.gov/assembly/' + accession\n",
    "        page = requests.get(url).content.decode()\n",
    "        accList = self.ncbiFilter(page)\n",
    "        return(accList)        \n",
    "            \n",
    "    def ncbiFilter(self, content): \n",
    "        startID = content.find('Organism name') \n",
    "        content = content[startID:]\n",
    "        endID = content.find('id=\"messagearea_bottom\">')-5\n",
    "        content = content[:endID]\n",
    "        extras = [': ']\n",
    "        content = self.webFilter(content, extras)\n",
    "        return(content)\n",
    "    \n",
    "    ### Filter out HTML formatting to make the information more human friendly\n",
    "    def webFilter(self, content, extras = []): \n",
    "        htmlStuff = ['</div>', '</span>', '</a>', '</dt>', '<dd>', '</dd>', '<dt>', '</h1>', '</li>', '<li>', \n",
    "                     '<td>', '</td>', '<tl>', '<tr>', '</tr>', '<td class=\"align_r\">','<span>', '<tbody>', '<em>', \n",
    "                     '<table>', '</tbody>', '</table>', '</dl>', 'ALT=', '</em>'] + extras\n",
    "        for littleString in htmlStuff: \n",
    "            content = content.replace(littleString, '\\n')\n",
    "        niceList = list()\n",
    "        safetynet = 0\n",
    "        while content.find('\\n') != -1 and safetynet < 10000: \n",
    "            nextLine = content.find('\\n')\n",
    "            blob = content[:nextLine]\n",
    "            if len(blob) > 0: \n",
    "                niceList.append(blob)\n",
    "            content = content[nextLine+1:]\n",
    "            safetynet = safetynet + 1  ## You can remove this but I have a fear of while loops\n",
    "        return(niceList)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
